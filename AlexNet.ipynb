{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "class AlexNet(object):\n",
    "    \n",
    "\n",
    "  \n",
    "    def __init__(self, x, keep_prob, num_classes, skip_layer, \n",
    "\n",
    "               weights_path = 'DEFAULT'):\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "    # Parse input arguments into class variables\n",
    "\n",
    "        self.X = x\n",
    "\n",
    "        self.NUM_CLASSES = num_classes\n",
    "\n",
    "        self.KEEP_PROB = keep_prob\n",
    "\n",
    "        self.SKIP_LAYER = skip_layer\n",
    "\n",
    "    \n",
    "\n",
    "        if weights_path == 'DEFAULT':      \n",
    "\n",
    "            self.WEIGHTS_PATH = 'bvlc_alexnet.npy'\n",
    "\n",
    "        else:\n",
    "\n",
    "            self.WEIGHTS_PATH = weights_path\n",
    "\n",
    "    \n",
    "\n",
    "        # Call the create function to build the computational graph of AlexNet\n",
    "\n",
    "    \n",
    "        self.create()\n",
    "\n",
    "    \n",
    "\n",
    "    def create(self):\n",
    "\n",
    "    \n",
    "\n",
    "        # 1st Layer: Conv (w ReLu) -> Pool -> Lrn\n",
    "\n",
    "        conv1 = conv(self.X, 11, 11, 96, 4, 4, padding = 'VALID', name = 'conv1')\n",
    "\n",
    "        pool1 = max_pool(conv1, 3, 3, 2, 2, padding = 'VALID', name = 'pool1')\n",
    "\n",
    "        norm1 = lrn(pool1, 2, 2e-05, 0.75, name = 'norm1')\n",
    "\n",
    "    \n",
    "\n",
    "        # 2nd Layer: Conv (w ReLu) -> Pool -> Lrn with 2 groups\n",
    "\n",
    "        conv2 = conv(norm1, 5, 5, 256, 1, 1, groups = 2, name = 'conv2')\n",
    "\n",
    "        pool2 = max_pool(conv2, 3, 3, 2, 2, padding = 'VALID', name ='pool2')\n",
    "\n",
    "        norm2 = lrn(pool2, 2, 2e-05, 0.75, name = 'norm2')\n",
    "\n",
    "    \n",
    "\n",
    "        # 3rd Layer: Conv (w ReLu)\n",
    "\n",
    "        conv3 = conv(norm2, 3, 3, 384, 1, 1, name = 'conv3')\n",
    "\n",
    "    \n",
    "\n",
    "        # 4th Layer: Conv (w ReLu) splitted into two groups\n",
    "\n",
    "        conv4 = conv(conv3, 3, 3, 384, 1, 1, groups = 2, name = 'conv4')\n",
    "\n",
    "    \n",
    "\n",
    "        # 5th Layer: Conv (w ReLu) -> Pool splitted into two groups\n",
    "\n",
    "        conv5 = conv(conv4, 3, 3, 256, 1, 1, groups = 2, name = 'conv5')\n",
    "\n",
    "        pool5 = max_pool(conv5, 3, 3, 2, 2, padding = 'VALID', name = 'pool5')\n",
    "\n",
    "    \n",
    "\n",
    "        # 6th Layer: Flatten -> FC (w ReLu) -> Dropout\n",
    "\n",
    "        flattened = tf.reshape(pool5, [-1, 6*6*256])\n",
    "\n",
    "        fc6 = fc(flattened, 6*6*256, 4096, name='fc6')\n",
    "\n",
    "        dropout6 = dropout(fc6, self.KEEP_PROB)\n",
    "\n",
    "    \n",
    "\n",
    "    # 7th Layer: FC (w ReLu) -> Dropout\n",
    "\n",
    "        fc7 = fc(dropout6, 4096, 4096, name = 'fc7')\n",
    "\n",
    "        dropout7 = dropout(fc7, self.KEEP_PROB)\n",
    "\n",
    "    \n",
    "\n",
    "    # 8th Layer: FC and return unscaled activations (for tf.nn.softmax_cross_entropy_with_logits)\n",
    "\n",
    "        self.fc8 = fc(dropout7, 4096, self.NUM_CLASSES, relu = False, name='fc8')\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    def load_initial_weights(self, session):\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        As the weights from http://www.cs.toronto.edu/~guerzhoy/tf_alexnet/ come \n",
    "\n",
    "        as a dict of lists (e.g. weights['conv1'] is a list) and not as dict of \n",
    "\n",
    "        dicts (e.g. weights['conv1'] is a dict with keys 'weights' & 'biases') we\n",
    "\n",
    "        need a special load function\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "    \n",
    "\n",
    "        # Load the weights into memory\n",
    "\n",
    "        weights_dict = np.load(self.WEIGHTS_PATH, encoding = 'bytes').item()\n",
    "\n",
    "    \n",
    "\n",
    "        # Loop over all layer names stored in the weights dict\n",
    "\n",
    "        for op_name in weights_dict:\n",
    "\n",
    "        \n",
    "\n",
    "            # Check if the layer is one of the layers that should be reinitialized\n",
    "\n",
    "            if op_name not in self.SKIP_LAYER:\n",
    "\n",
    "        \n",
    "\n",
    "                with tf.variable_scope(op_name, reuse = True):\n",
    "\n",
    "            \n",
    "\n",
    "                    # Loop over list of weights/biases and assign them to their corresponding tf variable\n",
    "\n",
    "                    for data in weights_dict[op_name]:\n",
    "\n",
    "            \n",
    "\n",
    "                        # Biases\n",
    "\n",
    "                        if len(data.shape) == 1:\n",
    "                            \n",
    "                            var = tf.get_variable('biases', trainable = False)\n",
    "\n",
    "                            session.run(var.assign(data))\n",
    "\n",
    "              \n",
    "\n",
    "                        # Weights\n",
    "\n",
    "                        else:\n",
    "                             \n",
    "                            var = tf.get_variable('weights', trainable = False)\n",
    "                             \n",
    "                            session.run(var.assign(data))\n",
    "\n",
    "            \n",
    "\n",
    "     \n",
    "\n",
    "  \n",
    "\n",
    " \n",
    "\"\"\"\n",
    "Predefine all necessary layer for the AlexNet\n",
    "\n",
    "\"\"\" \n",
    "\n",
    "def conv(x, filter_height, filter_width, num_filters, stride_y, stride_x, name,\n",
    "\n",
    "         padding='SAME', groups=1):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Adapted from: https://github.com/ethereon/caffe-tensorflow\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Get number of input channels\n",
    "\n",
    "    input_channels = int(x.get_shape()[-1])\n",
    "\n",
    "  \n",
    "\n",
    "    # Create lambda function for the convolution\n",
    "\n",
    "    convolve = lambda i, k: tf.nn.conv2d(i, k, \n",
    "\n",
    "                                       strides = [1, stride_y, stride_x, 1],\n",
    "\n",
    "                                       padding = padding)\n",
    "\n",
    "  \n",
    "\n",
    "    with tf.variable_scope(name) as scope:\n",
    "\n",
    "        # Create tf variables for the weights and biases of the conv layer\n",
    "\n",
    "        weights = tf.get_variable('weights', shape = [filter_height, filter_width, input_channels/groups, num_filters])\n",
    "\n",
    "        biases = tf.get_variable('biases', shape = [num_filters])  \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "        if groups == 1:\n",
    "\n",
    "            conv = convolve(x, weights)\n",
    "\n",
    "      \n",
    "\n",
    "        # In the cases of multiple groups, split inputs & weights and\n",
    "\n",
    "        else:\n",
    "\n",
    "            # Split input and weights and convolve them separately\n",
    "\n",
    "            input_groups = tf.split(axis = 3, num_or_size_splits=groups, value=x)\n",
    "\n",
    "            weight_groups = tf.split(axis = 3, num_or_size_splits=groups, value=weights)\n",
    "\n",
    "            output_groups = [convolve(i, k) for i,k in zip(input_groups, weight_groups)]\n",
    "\n",
    "      \n",
    "\n",
    "            # Concat the convolved output together again\n",
    "\n",
    "            conv = tf.concat(axis = 3, values = output_groups)\n",
    "\n",
    "      \n",
    "\n",
    "        # Add biases \n",
    "\n",
    "        bias = tf.reshape(tf.nn.bias_add(conv, biases), conv.get_shape().as_list())\n",
    "\n",
    "    \n",
    "\n",
    "        # Apply relu function\n",
    "\n",
    "        relu = tf.nn.relu(bias, name = scope.name)\n",
    "\n",
    "        \n",
    "\n",
    "    return relu\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "def fc(x, num_in, num_out, name, relu = True):\n",
    "\n",
    "    with tf.variable_scope(name) as scope:\n",
    "\n",
    "    \n",
    "\n",
    "        # Create tf variables for the weights and biases\n",
    "\n",
    "        weights = tf.get_variable('weights', shape=[num_in, num_out], trainable=True)\n",
    "\n",
    "        biases = tf.get_variable('biases', [num_out], trainable=True)\n",
    "\n",
    "    \n",
    "\n",
    "        # Matrix multiply weights and inputs and add bias\n",
    "\n",
    "        act = tf.nn.xw_plus_b(x, weights, biases, name=scope.name)\n",
    "\n",
    "    \n",
    "\n",
    "        if relu == True:\n",
    "\n",
    "            # Apply ReLu non linearity\n",
    "\n",
    "            relu = tf.nn.relu(act)      \n",
    "\n",
    "            return relu\n",
    "\n",
    "        else:\n",
    "\n",
    "            return act\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def max_pool(x, filter_height, filter_width, stride_y, stride_x, name, padding='SAME'):\n",
    "\n",
    "    return tf.nn.max_pool(x, ksize=[1, filter_height, filter_width, 1],\n",
    "\n",
    "                        strides = [1, stride_y, stride_x, 1],\n",
    "\n",
    "                        padding = padding, name = name)\n",
    "\n",
    "  \n",
    "\n",
    "def lrn(x, radius, alpha, beta, name, bias=1.0):\n",
    "\n",
    "    return tf.nn.local_response_normalization(x, depth_radius = radius, alpha = alpha,\n",
    "\n",
    "                                            beta = beta, bias = bias, name = name)\n",
    "\n",
    "  \n",
    "\n",
    "def dropout(x, keep_prob):\n",
    "\n",
    "    return tf.nn.dropout(x, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
