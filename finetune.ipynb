{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from alexnet import AlexNet\n",
    "from datagenerator import ImageDataGenerator\n",
    "\n",
    "\"\"\"\n",
    "Configuration settings\n",
    "\"\"\"\n",
    "\n",
    "# Path to the textfiles for the trainings and validation set\n",
    "\n",
    "train_file = 'train.txt'\n",
    "val_file = 'val.txt'\n",
    "\n",
    "\n",
    "# Learning params\n",
    "learning_rate = 0.01\n",
    "num_epochs = 5\n",
    "batch_size = 5\n",
    "\n",
    "# Network params\n",
    "dropout_rate = 0.5\n",
    "num_classes = 4\n",
    "train_layers = ['fc8', 'fc7']\n",
    "\n",
    "# How often we want to write the tf.summary data to disk\n",
    "display_step = 1\n",
    "\n",
    "# Path for tf.summary.FileWriter and to store model checkpoints\n",
    "filewriter_path = \"C:/Users/Jiankun/Desktop/AlexNet/filewriter\"\n",
    "checkpoint_path = \"C:/Users/Jiankun/Desktop/AlexNet/checkpoint\"\n",
    "\n",
    "# Create parent path if it doesn't exist\n",
    "if not os.path.isdir(checkpoint_path): os.mkdir(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TF placeholder for graph input and output\n",
    "x = tf.placeholder(tf.float32, [batch_size, 227, 227, 3])\n",
    "y = tf.placeholder(tf.float32, [None, num_classes])\n",
    "keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tf.reset_default_graph()\n",
    "\n",
    "# Initialize model\n",
    "model = AlexNet(x, keep_prob, num_classes, train_layers)\n",
    "\n",
    "# Link variable to model output\n",
    "score = model.fc8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name fc7/weights:0/gradient is illegal; using fc7/weights_0/gradient instead.\n",
      "INFO:tensorflow:Summary name fc7/biases:0/gradient is illegal; using fc7/biases_0/gradient instead.\n",
      "INFO:tensorflow:Summary name fc8/weights:0/gradient is illegal; using fc8/weights_0/gradient instead.\n",
      "INFO:tensorflow:Summary name fc8/biases:0/gradient is illegal; using fc8/biases_0/gradient instead.\n",
      "INFO:tensorflow:Summary name fc7/weights:0 is illegal; using fc7/weights_0 instead.\n",
      "INFO:tensorflow:Summary name fc7/biases:0 is illegal; using fc7/biases_0 instead.\n",
      "INFO:tensorflow:Summary name fc8/weights:0 is illegal; using fc8/weights_0 instead.\n",
      "INFO:tensorflow:Summary name fc8/biases:0 is illegal; using fc8/biases_0 instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'cross_entropy:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of trainable variables of the layers we want to train\n",
    "var_list = [v for v in tf.trainable_variables() if v.name.split('/')[0] in train_layers]\n",
    "\n",
    "# Op for calculating the loss\n",
    "with tf.name_scope(\"cross_ent\"):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = score, labels = y))  \n",
    "\n",
    "# Train op\n",
    "with tf.name_scope(\"train\"):\n",
    "    # Get gradients of all trainable variables\n",
    "    gradients = tf.gradients(loss, var_list)\n",
    "    gradients = list(zip(gradients, var_list))\n",
    "  \n",
    "    # Create optimizer and apply gradient descent to the trainable variables\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    train_op = optimizer.apply_gradients(grads_and_vars=gradients)\n",
    "\n",
    "# Add gradients to summary  \n",
    "for gradient, var in gradients:\n",
    "    tf.summary.histogram(var.name + '/gradient', gradient)\n",
    "\n",
    "# Add the variables we train to the summary  \n",
    "for var in var_list:\n",
    "    tf.summary.histogram(var.name, var)\n",
    "\n",
    "    \n",
    "# Add the loss to summary\n",
    "tf.summary.scalar('cross_entropy', loss)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluation op: Accuracy of the model\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_pred = tf.equal(tf.argmax(score, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Add the accuracy to the summary\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "# Merge all summaries together\n",
    "merged_summary = tf.summary.merge_all()\n",
    "\n",
    "# Initialize the FileWriter\n",
    "writer = tf.summary.FileWriter(filewriter_path)\n",
    "\n",
    "# Initialize an saver for store model checkpoints\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Initalize the data generator seperately for the training and validation set\n",
    "train_generator = ImageDataGenerator(train_file, \n",
    "                                     horizontal_flip = True, shuffle = True, nb_classes = num_classes)\n",
    "val_generator = ImageDataGenerator(val_file, shuffle = False, nb_classes = num_classes) \n",
    "\n",
    "# Get the number of training/validation steps per epoch\n",
    "train_batches_per_epoch = np.floor(train_generator.data_size / batch_size).astype(np.int16)\n",
    "val_batches_per_epoch = np.floor(val_generator.data_size / batch_size).astype(np.int16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-04 16:15:39.402071 Start training...\n",
      "2017-11-04 16:15:39.402071 Open Tensorboard at --logdir C:/Users/Jiankun/Desktop/AlexNet/filewriter\n",
      "2017-11-04 16:15:39.402071 Epoch number: 1\n",
      "2017-11-04 16:15:55.345748 Start validation\n",
      "2017-11-04 16:15:56.023641 Validation Accuracy = 0.3000\n",
      "2017-11-04 16:15:56.024142 Saving checkpoint of model...\n",
      "2017-11-04 16:16:00.300105 Model checkpoint saved at C:/Users/Jiankun/Desktop/AlexNet/checkpoint\\model_epoch1.ckpt\n",
      "2017-11-04 16:16:00.300105 Epoch number: 2\n",
      "2017-11-04 16:16:17.299408 Start validation\n",
      "2017-11-04 16:16:17.971194 Validation Accuracy = 0.2000\n",
      "2017-11-04 16:16:17.971695 Saving checkpoint of model...\n",
      "2017-11-04 16:16:22.968850 Model checkpoint saved at C:/Users/Jiankun/Desktop/AlexNet/checkpoint\\model_epoch2.ckpt\n",
      "2017-11-04 16:16:22.968850 Epoch number: 3\n",
      "2017-11-04 16:16:38.429826 Start validation\n",
      "2017-11-04 16:16:39.061637 Validation Accuracy = 0.3000\n",
      "2017-11-04 16:16:39.062138 Saving checkpoint of model...\n",
      "2017-11-04 16:16:44.661609 Model checkpoint saved at C:/Users/Jiankun/Desktop/AlexNet/checkpoint\\model_epoch3.ckpt\n",
      "2017-11-04 16:16:44.661609 Epoch number: 4\n",
      "2017-11-04 16:16:59.976044 Start validation\n",
      "2017-11-04 16:17:00.630975 Validation Accuracy = 0.3000\n",
      "2017-11-04 16:17:00.631477 Saving checkpoint of model...\n",
      "2017-11-04 16:17:05.210198 Model checkpoint saved at C:/Users/Jiankun/Desktop/AlexNet/checkpoint\\model_epoch4.ckpt\n",
      "2017-11-04 16:17:05.210198 Epoch number: 5\n",
      "2017-11-04 16:17:20.526737 Start validation\n",
      "2017-11-04 16:17:21.161056 Validation Accuracy = 0.2000\n",
      "2017-11-04 16:17:21.161056 Saving checkpoint of model...\n",
      "2017-11-04 16:17:26.013903 Model checkpoint saved at C:/Users/Jiankun/Desktop/AlexNet/checkpoint\\model_epoch5.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Start Tensorflow session\n",
    "with tf.Session() as sess:\n",
    " \n",
    "    # Initialize all variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "  \n",
    "    # Add the model graph to TensorBoard\n",
    "    writer.add_graph(sess.graph)\n",
    "  \n",
    "    # Load the pretrained weights into the non-trainable layer\n",
    "    model.load_initial_weights(sess)\n",
    "  \n",
    "    print(\"{} Start training...\".format(datetime.now()))\n",
    "    print(\"{} Open Tensorboard at --logdir {}\".format(datetime.now(), \n",
    "                                                    filewriter_path))\n",
    "  \n",
    "    # Loop over number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "    \n",
    "        print(\"{} Epoch number: {}\".format(datetime.now(), epoch+1))\n",
    "        \n",
    "        step = 1\n",
    "        \n",
    "        while step < train_batches_per_epoch:\n",
    "            \n",
    "            # Get a batch of images and labels\n",
    "            batch_xs, batch_ys = train_generator.next_batch(batch_size)\n",
    "            \n",
    "            # And run the training op\n",
    "            sess.run(train_op, feed_dict={x: batch_xs, \n",
    "                                          y: batch_ys, \n",
    "                                          keep_prob: dropout_rate})\n",
    "            \n",
    "            # Generate summary with the current batch of data and write to file\n",
    "            if step%display_step == 0:\n",
    "                s = sess.run(merged_summary, feed_dict={x: batch_xs, \n",
    "                                                        y: batch_ys, \n",
    "                                                        keep_prob: 1.})\n",
    "                writer.add_summary(s, epoch*train_batches_per_epoch + step)\n",
    "                \n",
    "            step += 1\n",
    "            \n",
    "        # Validate the model on the entire validation set\n",
    "        print(\"{} Start validation\".format(datetime.now()))\n",
    "        test_acc = 0.\n",
    "        test_count = 0\n",
    "        for _ in range(val_batches_per_epoch):\n",
    "            batch_tx, batch_ty = val_generator.next_batch(batch_size)\n",
    "            acc = sess.run(accuracy, feed_dict={x: batch_tx, \n",
    "                                                y: batch_ty, \n",
    "                                                keep_prob: 1.})\n",
    "            test_acc += acc\n",
    "            test_count += 1\n",
    "        test_acc /= test_count\n",
    "        print(\"{} Validation Accuracy = {:.4f}\".format(datetime.now(), test_acc))\n",
    "        \n",
    "        # Reset the file pointer of the image data generator\n",
    "        val_generator.reset_pointer()\n",
    "        train_generator.reset_pointer()\n",
    "        \n",
    "        print(\"{} Saving checkpoint of model...\".format(datetime.now()))  \n",
    "        \n",
    "        #save checkpoint of the model\n",
    "        checkpoint_name = os.path.join(checkpoint_path, 'model_epoch'+str(epoch+1)+'.ckpt')\n",
    "        save_path = saver.save(sess, checkpoint_name)  \n",
    "        \n",
    "        print(\"{} Model checkpoint saved at {}\".format(datetime.now(), checkpoint_name))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
